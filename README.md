# NLP and LLM Project Portfolio

Welcome to my portfolio of projects focused on Natural Language Processing (NLP) and Large Language Models (LLMs). This repository showcases my hands-on experience with foundational models like BERT and GPT, and my ability to apply them to practical tasks such as sentiment analysis and text generation.

---

## Projects

Below is a summary of the projects included in this repository. Each project is contained within its own Jupyter Notebook.

### 1. BERT Fine-Tuning for Sentiment Analysis
* **File:** `BERT_Fine-Tuning_for_Sentiment_Analysis.ipynb`
* **Description:** This project demonstrates the process of fine-tuning a pre-trained BERT model for a downstream task. The model is trained on a dataset of movie reviews to classify sentiment as either positive or negative, achieving high accuracy.
* **Key Skills:** Transfer Learning, Sentiment Analysis, BERT, Hugging Face Transformers, PyTorch.

### 2. GPT Text Generation with Transformers
* **File:** `GPT_Text_Generation_with_Transformers.ipynb`
* **Description:** A deep dive into using the GPT-2 model for causal language modeling. This notebook explores how to generate coherent and contextually relevant text by leveraging the power of the Transformer architecture.
* **Key Skills:** Text Generation, GPT-2, Causal Language Modeling, Transformers, Tokenization.

### 3. IMDB Sentiment Analysis with a Custom LLM
* **File:** `IMDB_Sentiment_Analysis_with_LLM.ipynb`
* **Description:** This project focuses on building a sentiment analysis model for the IMDB dataset from a more foundational level. It covers text preprocessing, vectorization, and training a machine learning classifier to predict sentiment.
* **Key Skills:** Text Preprocessing (Regex, Stop Words), Word Embeddings (Word2Vec), Scikit-learn, NLP Pipelines.

### 4. BERT vs. AutoModel Comparison
* **File:** `BERT_vs_AutoModel_Comparison.ipynb`
* **Description:** An exploratory notebook that compares the direct use of `BertModel` with the flexible `AutoModel` class from the Hugging Face library. This project highlights the practical advantages of using the AutoModel paradigm for loading different transformer architectures.
* **Key Skills:** Hugging Face `AutoModel`, `AutoTokenizer`, BERT, Model Loading, API Comparison.
